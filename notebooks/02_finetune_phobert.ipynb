{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3f5b75",
   "metadata": {},
   "source": [
    "# ü§ñ B√†i th·ª±c h√†nh 2: Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i b·ªánh v·ªõi PhoBERT\n",
    "Trong b√†i n√†y, sinh vi√™n s·∫Ω fine-tune m√¥ h√¨nh PhoBERT ƒë·ªÉ d·ª± ƒëo√°n b·ªánh t·ª´ c√¢u m√¥ t·∫£ tri·ªáu ch·ª©ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d937a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2a23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e78b15c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels_list",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "binary_labels",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bc75ef7e-da69-45d9-853a-31b40e36d1c5",
       "rows": [
        [
         "0",
         "T√¥i b·ªã ho v√† s·ªët su·ªët ba ng√†y nay.",
         "['vi√™m ph·ªïi', 's·ªët virus']",
         "['vi√™m ph·ªïi', 's·ªët virus']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]"
        ],
        [
         "1",
         "T√¥i th·∫•y ƒëau b·ª•ng d·ªØ d·ªôi v√† ti√™u ch·∫£y.",
         "['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']",
         "['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
        ],
        [
         "2",
         "T√¥i th∆∞·ªùng xuy√™n ch√≥ng m·∫∑t v√† nh·ª©c ƒë·∫ßu.",
         "['thi·∫øu m√°u', 'cao huy·∫øt √°p']",
         "['thi·∫øu m√°u', 'cao huy·∫øt √°p']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
        ],
        [
         "3",
         "Kh√≥ th·ªü, ho khan v√† c·∫£m th·∫•y m·ªát m·ªèi.",
         "['covid-19', 'vi√™m ph·ªïi']",
         "['covid-19', 'vi√™m ph·ªïi']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]"
        ],
        [
         "4",
         "T√¥i b·ªã ng·ª©a, n√¥n, m·ªát m·ªèi, s·ª•t c√¢n, s·ªët cao, da v√†ng, n∆∞·ªõc ti·ªÉu s·∫´m m√†u, ƒëau b·ª•ng.",
         "['V√†ng da']",
         "['V√†ng da']",
         "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_list</th>\n",
       "      <th>binary_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T√¥i b·ªã ho v√† s·ªët su·ªët ba ng√†y nay.</td>\n",
       "      <td>['vi√™m ph·ªïi', 's·ªët virus']</td>\n",
       "      <td>['vi√™m ph·ªïi', 's·ªët virus']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T√¥i th·∫•y ƒëau b·ª•ng d·ªØ d·ªôi v√† ti√™u ch·∫£y.</td>\n",
       "      <td>['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']</td>\n",
       "      <td>['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T√¥i th∆∞·ªùng xuy√™n ch√≥ng m·∫∑t v√† nh·ª©c ƒë·∫ßu.</td>\n",
       "      <td>['thi·∫øu m√°u', 'cao huy·∫øt √°p']</td>\n",
       "      <td>['thi·∫øu m√°u', 'cao huy·∫øt √°p']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kh√≥ th·ªü, ho khan v√† c·∫£m th·∫•y m·ªát m·ªèi.</td>\n",
       "      <td>['covid-19', 'vi√™m ph·ªïi']</td>\n",
       "      <td>['covid-19', 'vi√™m ph·ªïi']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√¥i b·ªã ng·ª©a, n√¥n, m·ªát m·ªèi, s·ª•t c√¢n, s·ªët cao, d...</td>\n",
       "      <td>['V√†ng da']</td>\n",
       "      <td>['V√†ng da']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                 T√¥i b·ªã ho v√† s·ªët su·ªët ba ng√†y nay.   \n",
       "1             T√¥i th·∫•y ƒëau b·ª•ng d·ªØ d·ªôi v√† ti√™u ch·∫£y.   \n",
       "2            T√¥i th∆∞·ªùng xuy√™n ch√≥ng m·∫∑t v√† nh·ª©c ƒë·∫ßu.   \n",
       "3              Kh√≥ th·ªü, ho khan v√† c·∫£m th·∫•y m·ªát m·ªèi.   \n",
       "4  T√¥i b·ªã ng·ª©a, n√¥n, m·ªát m·ªèi, s·ª•t c√¢n, s·ªët cao, d...   \n",
       "\n",
       "                          labels                    labels_list  \\\n",
       "0     ['vi√™m ph·ªïi', 's·ªët virus']     ['vi√™m ph·ªïi', 's·ªët virus']   \n",
       "1          ['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']          ['ng·ªô ƒë·ªôc th·ª±c ph·∫©m']   \n",
       "2  ['thi·∫øu m√°u', 'cao huy·∫øt √°p']  ['thi·∫øu m√°u', 'cao huy·∫øt √°p']   \n",
       "3      ['covid-19', 'vi√™m ph·ªïi']      ['covid-19', 'vi√™m ph·ªïi']   \n",
       "4                    ['V√†ng da']                    ['V√†ng da']   \n",
       "\n",
       "                                       binary_labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B∆∞·ªõc 1: Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
    "df = pd.read_csv('/Users/dinhquanghien/Documents/H·ªçc t·∫≠p/pre_2/module/processed_medical_chat_dataset.csv')\n",
    "df['binary_labels'] = df['binary_labels'].apply(eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "680548ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 2: ƒê·ªãnh nghƒ©a Dataset\n",
    "class MedicalChatDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ac8656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 3: ƒê·ªãnh nghƒ©a m√¥ h√¨nh\n",
    "class PhoBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(PhoBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0]\n",
    "        x = self.dropout(pooled_output)\n",
    "        return torch.sigmoid(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94165038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 4: Kh·ªüi t·∫°o DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['binary_labels'], test_size=0.2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "train_dataset = MedicalChatDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
    "val_dataset = MedicalChatDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f4b17b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: mps\n",
      "\n",
      "üìä Class imbalance statistics:\n",
      "   Min positive samples: 1\n",
      "   Max positive samples: 83\n",
      "   Avg positive samples: 28.0\n",
      "   Min weight: 27.95\n",
      "   Max weight: 2403.00\n",
      "   Avg weight: 1237.58\n",
      "\n",
      "\n",
      "üöÄ Starting training with Focal Loss...\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "   üîç Debug (threshold=0.2):\n",
      "      Avg prob:        0.1614 (target: ~0.0116)\n",
      "      Max prob:        0.7889\n",
      "      P75 prob:        0.1893\n",
      "      P95 prob:        0.2685\n",
      "      Positive preds:  10519 (target: 601)\n",
      "\n",
      "üìä Epoch 1/5\n",
      "   Train Loss:       0.0083\n",
      "   Val Loss:         0.0049\n",
      "   Hamming Acc:      0.8078\n",
      "   Precision (micro): 0.0565\n",
      "   Recall (micro):    0.9884\n",
      "   F1 (micro):        0.1068\n",
      "   F1 (samples):      0.1097\n",
      "====================================================================================================\n",
      "   ‚úÖ Best model saved! (F1-micro: 0.1068)\n",
      "\n",
      "   üîç Debug (threshold=0.2):\n",
      "      Avg prob:        0.1179 (target: ~0.0116)\n",
      "      Max prob:        0.8814\n",
      "      P75 prob:        0.1336\n",
      "      P95 prob:        0.1873\n",
      "      Positive preds:  1982 (target: 601)\n",
      "\n",
      "üìä Epoch 2/5\n",
      "   Train Loss:       0.0020\n",
      "   Val Loss:         0.0019\n",
      "   Hamming Acc:      0.9730\n",
      "   Precision (micro): 0.2992\n",
      "   Recall (micro):    0.9867\n",
      "   F1 (micro):        0.4592\n",
      "   F1 (samples):      0.6678\n",
      "====================================================================================================\n",
      "   ‚úÖ Best model saved! (F1-micro: 0.4592)\n",
      "\n",
      "   üîç Debug (threshold=0.2):\n",
      "      Avg prob:        0.0971 (target: ~0.0116)\n",
      "      Max prob:        0.9148\n",
      "      P75 prob:        0.1069\n",
      "      P95 prob:        0.1516\n",
      "      Positive preds:  1098 (target: 601)\n",
      "\n",
      "üìä Epoch 3/5\n",
      "   Train Loss:       0.0009\n",
      "   Val Loss:         0.0011\n",
      "   Hamming Acc:      0.9901\n",
      "   Precision (micro): 0.5401\n",
      "   Recall (micro):    0.9867\n",
      "   F1 (micro):        0.6981\n",
      "   F1 (samples):      0.8400\n",
      "====================================================================================================\n",
      "   ‚úÖ Best model saved! (F1-micro: 0.6981)\n",
      "\n",
      "   üîç Debug (threshold=0.2):\n",
      "      Avg prob:        0.0786 (target: ~0.0116)\n",
      "      Max prob:        0.9053\n",
      "      P75 prob:        0.0836\n",
      "      P95 prob:        0.1170\n",
      "      Positive preds:  951 (target: 601)\n",
      "\n",
      "üìä Epoch 4/5\n",
      "   Train Loss:       0.0005\n",
      "   Val Loss:         0.0007\n",
      "   Hamming Acc:      0.9929\n",
      "   Precision (micro): 0.6225\n",
      "   Recall (micro):    0.9850\n",
      "   F1 (micro):        0.7629\n",
      "   F1 (samples):      0.9037\n",
      "====================================================================================================\n",
      "   ‚úÖ Best model saved! (F1-micro: 0.7629)\n",
      "\n",
      "   üîç Debug (threshold=0.2):\n",
      "      Avg prob:        0.0700 (target: ~0.0116)\n",
      "      Max prob:        0.9257\n",
      "      P75 prob:        0.0719\n",
      "      P95 prob:        0.1039\n",
      "      Positive preds:  856 (target: 601)\n",
      "\n",
      "üìä Epoch 5/5\n",
      "   Train Loss:       0.0003\n",
      "   Val Loss:         0.0006\n",
      "   Hamming Acc:      0.9946\n",
      "   Precision (micro): 0.6893\n",
      "   Recall (micro):    0.9817\n",
      "   F1 (micro):        0.8099\n",
      "   F1 (samples):      0.9250\n",
      "====================================================================================================\n",
      "   ‚úÖ Best model saved! (F1-micro: 0.8099)\n",
      "\n",
      "üéâ Training completed! Best F1-micro: 0.8099\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Focal Loss v√† Class Weights\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "# S·ª≠ d·ª•ng MPS (Metal Performance Shaders) cho Mac M1/M2/M3\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Calculate positive class weights from training data\n",
    "train_labels_array = np.array(train_labels.tolist())\n",
    "pos_counts = train_labels_array.sum(axis=0)\n",
    "neg_counts = len(train_labels_array) - pos_counts\n",
    "\n",
    "# Avoid division by zero\n",
    "pos_counts = np.where(pos_counts == 0, 1, pos_counts)\n",
    "\n",
    "# Calculate weights: neg/pos ratio (higher weight for rare classes)\n",
    "pos_weights = neg_counts / pos_counts\n",
    "pos_weights = torch.FloatTensor(pos_weights).to(device)\n",
    "\n",
    "print(f\"\\nüìä Class imbalance statistics:\")\n",
    "print(f\"   Min positive samples: {pos_counts.min():.0f}\")\n",
    "print(f\"   Max positive samples: {pos_counts.max():.0f}\")\n",
    "print(f\"   Avg positive samples: {pos_counts.mean():.1f}\")\n",
    "print(f\"   Min weight: {pos_weights.min():.2f}\")\n",
    "print(f\"   Max weight: {pos_weights.max():.2f}\")\n",
    "print(f\"   Avg weight: {pos_weights.mean():.2f}\\n\")\n",
    "\n",
    "model = PhoBERTClassifier(num_labels=len(train_labels.iloc[0])).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)  # Increased LR\n",
    "\n",
    "# Use weighted BCE with pos_weights\n",
    "criterion = nn.BCELoss(reduction='none')  # We'll apply weights manually\n",
    "\n",
    "def focal_loss(outputs, targets, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"Focal loss for handling class imbalance\"\"\"\n",
    "    bce_loss = nn.functional.binary_cross_entropy(outputs, targets, reduction='none')\n",
    "    pt = torch.where(targets == 1, outputs, 1 - outputs)\n",
    "    focal_weight = (alpha * targets + (1 - alpha) * (1 - targets)) * ((1 - pt) ** gamma)\n",
    "    loss = focal_weight * bce_loss\n",
    "    return loss.mean()\n",
    "\n",
    "def train_epoch(model, loader, device):\n",
    "    \"\"\"Train for one epoch with focal loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Use focal loss\n",
    "        loss = focal_loss(outputs, labels, alpha=0.75, gamma=2.0)  # High alpha for positive class\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device, threshold=0.2):\n",
    "    \"\"\"Evaluate with adaptive threshold\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = focal_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_probs.append(outputs.cpu().numpy())\n",
    "            preds = (outputs > threshold).float()\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Debug info\n",
    "    print(f\"\\n   üîç Debug (threshold={threshold}):\")\n",
    "    print(f\"      Avg prob:        {all_probs.mean():.4f} (target: ~{all_labels.mean():.4f})\")\n",
    "    print(f\"      Max prob:        {all_probs.max():.4f}\")\n",
    "    print(f\"      P75 prob:        {np.percentile(all_probs, 75):.4f}\")\n",
    "    print(f\"      P95 prob:        {np.percentile(all_probs, 95):.4f}\")\n",
    "    print(f\"      Positive preds:  {all_preds.sum():.0f} (target: {all_labels.sum():.0f})\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'exact_match': accuracy_score(all_labels, all_preds),\n",
    "        'hamming_acc': 1 - hamming_loss(all_labels, all_preds),\n",
    "        'precision_micro': precision_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "        'recall_micro': recall_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "        'f1_micro': f1_score(all_labels, all_preds, average='micro', zero_division=0),\n",
    "        'f1_samples': f1_score(all_labels, all_preds, average='samples', zero_division=0),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Training with learning rate warmup\n",
    "print(\"\\nüöÄ Starting training with Focal Loss...\\n\")\n",
    "print(\"=\" * 100)\n",
    "best_f1 = -1.0\n",
    "epochs = 5\n",
    "threshold = 0.2  # Lower threshold\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, device)\n",
    "    val_metrics = evaluate(model, val_loader, device, threshold=threshold)\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"   Train Loss:       {train_loss:.4f}\")\n",
    "    print(f\"   Val Loss:         {val_metrics['loss']:.4f}\")\n",
    "    print(f\"   Hamming Acc:      {val_metrics['hamming_acc']:.4f}\")\n",
    "    print(f\"   Precision (micro): {val_metrics['precision_micro']:.4f}\")\n",
    "    print(f\"   Recall (micro):    {val_metrics['recall_micro']:.4f}\")\n",
    "    print(f\"   F1 (micro):        {val_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"   F1 (samples):      {val_metrics['f1_samples']:.4f}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    metric_to_track = val_metrics['f1_micro'] if val_metrics['f1_micro'] > 0 else val_metrics['hamming_acc']\n",
    "    \n",
    "    if metric_to_track > best_f1:\n",
    "        best_f1 = metric_to_track\n",
    "        torch.save(model.state_dict(), 'phobert_medchat_model_best.pt')\n",
    "        print(f\"   ‚úÖ Best model saved! (F1-micro: {val_metrics['f1_micro']:.4f})\")\n",
    "\n",
    "print(f\"\\nüéâ Training completed! Best F1-micro: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 6: L∆∞u v√† ki·ªÉm tra m√¥ h√¨nh\n",
    "import os\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ m√¥ h√¨nh best kh√¥ng\n",
    "if os.path.exists('phobert_medchat_model_best.pt'):\n",
    "    print(\"‚úÖ M√¥ h√¨nh t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: phobert_medchat_model_best.pt\")\n",
    "    best_model_path = 'phobert_medchat_model_best.pt'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y best model, s·ª≠ d·ª•ng m√¥ h√¨nh cu·ªëi c√πng\")\n",
    "    # L∆∞u m√¥ h√¨nh cu·ªëi c√πng\n",
    "    torch.save(model.state_dict(), 'phobert_medchat_model_last.pt')\n",
    "    print(\"‚úÖ M√¥ h√¨nh cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: phobert_medchat_model_last.pt\")\n",
    "    best_model_path = 'phobert_medchat_model_last.pt'\n",
    "\n",
    "# Test inference\n",
    "print(f\"\\nüß™ Testing inference v·ªõi m√¥ h√¨nh: {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "test_text = \"T√¥i b·ªã ho, s·ªët v√† kh√≥ th·ªü\"\n",
    "test_inputs = tokenizer(test_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "test_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
    "    test_probs = test_output.cpu().numpy()[0]\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    top_5_indices = np.argsort(test_probs)[-5:][::-1]\n",
    "    \n",
    "    print(f\"\\nInput: '{test_text}'\")\n",
    "    print(f\"Top 5 predictions:\")\n",
    "    for idx in top_5_indices:\n",
    "        print(f\"   Label {idx}: {test_probs[idx]:.4f}\")\n",
    "    \n",
    "    # Binary predictions with threshold\n",
    "    test_preds = (test_output > 0.3).float().cpu().numpy()[0]\n",
    "    predicted_labels = [i for i, val in enumerate(test_preds) if val == 1]\n",
    "    print(f\"\\nPredicted label indices (threshold=0.3): {predicted_labels}\")\n",
    "    \n",
    "print(\"‚úÖ Model inference ho·∫°t ƒë·ªông t·ªët!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
